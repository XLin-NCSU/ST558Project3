---
title: "Project 3"
output: github_document
author: Xi Lin, Sarat Bantupalli
params: 
  Education: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, message = FALSE, warning = FALSE)
```

### This report is for Education level `r params$Education`.

## Required Packages

```{r packages, echo=FALSE}
library(tidyverse)
library(corrplot)
library(caret)
```


## Introduction
This project creates predictive models using available data and automates R Markdown reports. We demonstrated it by using [Diabates Health Indicators Data](https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset) collected by the Centers of Disease Control and Prevention (CDC).  

The data was collected through a telephonic survey by the CDC in the year 2015 and it corresponds to the prevalence of diabetes in the survey respondents. The data set we analyzed here represents responses from 253,680 Americans on health-related risk behaviors, chronic health conditions, and the use of preventative services. The data set itself is *not balanced*.  

### Model Variables
The response variable of interest from the data set is *Diabetes_binary*. It represents if the survey respondent is *non diabetic (value 0)* or *pre-diabetic/ diabetic (value 1)*.

The explanatory variables were chosen such that important risk factors for diabetes including health & lifestyle, socioeconomic status, and demographics (age and sex) are considered in the predictive model. We included the following explanatory variables for the model:  

+ *HighBP*: This variable represents if a respondent was diagnosed with high blood pressure (value 1) or not (value 0). Research indicates high blood pressure is twice as likely to strike a person with diabetes than a person without diabetes  
+ *HighChol*: This variable represents if a respondent has high cholesterol (value 1) or not (value 0). Research suggests people with diabetes tend to have higher cholesterol levels and was included in the model  
+ *BMI*: The body mass index, a numerical variable, is a good indicator for obesity and represents the overall health of a person making it an ideal candidate for the model  
+ *Smoker*: Smoking increases the risk of type 2 diabetes and hence was included in the model. This variable can take two values, 0- did not smoke 100 cigarettes in lifetime or 1- smoked more than 100 cigarettes in lifetime  
+ *PhysicalActivity*: This variable indicates if the respondent had been physically active in the past 30 days. A value 0 represents no while 1 represents yes  
+ *Fruits*: A healthy diet is key to reducing the risk of type 2 diabetes and can represent lifestyle and socioeconomic status of the respondent. This variable indicates if the person is eating healthy or not. A value of 0 indicates no fruit consumed per day while 1 indicates some fruit consumed per day  
+ *Veggies*: This variable indicates if the person is eating healthy or not. It can represent lifetsyle and socioeconomic status of the respondent. A value of 0 indicates no veggies consumed per day while 1 indicates some veggies consumed per day  
+ *HvyAlcoholConsump*: Alcohol increases the risk of type 2 diabetes and this variable encapsulates if the respondent is a heavy drinker  
+ *AnyHealthcare*: This variable represents socioeconomic status of the respondent, with low-income respondents not having any health coverage (value 0) while higher income respondents having some type of health coverage (value 1)  
+ *GenHlth*: This represents the overall health of the respondent ranging from 1 through 5 with 1 being the best health indicator  
+ *MentHlth*: This numerical variable represents days of poor mental health (1-30 days). People with mental health are 2 to 3 times more likely to have a depression than people without diabetes. This variable captures the overall health of the respondent  
+ *PhysHlth*: This numerical variable indicates physical illness or injury days in past 30 days. Illness or injury can impact physical activity which in turn could affect type 2 diabetes risk  
+ *Sex*: Including the sex of the respondent (0- female, 1- male) could help us see if risk factors are different for men and women  
+ *Age*: Risk for type 2 diabetes increases with age. This 13 level categorical variable encapsulates this risk  
+ *Education*: Education might effect the socioeconomic status of a person and change the risk factor for type 2 diabetes. This variable was used to automate the R Markdown reports. Each report represents analysis for each level of this variable. A value of 2 indicates the respondent attended elementary school or less while 6 represents a college graduate  
+ *Income*: This categorical variable can effect a lot risks for type 2 diabetes including access to health care, physical activity (eg. gym), and or healthy lifestyle. A value of 1 indicates low income (less than $10,000), and a value of 8 indicates high income (>$75,000)  

### Purpose of Modeling

describes the purpose of your EDA and modeling, along with the end result you will be creating (see
below).


## Data Processing
The data for the analysis was read using the `read_csv` function from the `readr` package and stored in the R object, *diabetes*. Then for the variable *Education*, groups 1 and 2 were combined into *group 2*. This was done to have five distinct groups for this variable ranging from 2 through 5.  

Then some of the explanatory variables we are interested in were converted to factors. Variables that were converted to factors with meaningful level names include *HighBP*, *HighChol*,  

The data was then subset to represent different *Education* levels of the respondents in the survey.

```{r data_processing}
#Read in data using the read_csv function from readr package
diabetes <- read_csv("diabetes_binary_health_indicators_BRFSS2015.csv")

#Combine groups 1 and 2 into group 2 for Education variable
diabetes <- diabetes %>% 
  mutate(Education = if_else(Education == 1, 2, Education) %>% as.factor())


diabetes$Diabetes_binary <- if_else(diabetes$Diabetes_binary == 0, 
                                    "No_Diabetes","Diabetes")

# Create a vector with the columns we want to convert to factors
columns_to_factor <- c("Diabetes_binary",
                       "HighBP",
                       "HighChol",
                       "Smoker",
                       "PhysActivity",
                       "Fruits",
                       "Veggies",
                       "HvyAlcoholConsump",
                       "AnyHealthcare",
                       "GenHlth",
                       "Sex",
                       "Age",
                       "Income")

# Change the columns we are interested in to a factor using the lapply function
diabetes[, columns_to_factor] <- lapply(diabetes[, columns_to_factor], factor)

# Subset the data to work on the Education level of interest
#Diabetes_Edu <- subset(diabetes, Education == params$Education)
diabetes_education_level <- subset(diabetes, Education == params$Education) %>% 
  select(-Education)
```

## Summarizations and Exploratory Data Analysis (EDA)
Here Exploratory Data Analysis on the full data (for the particular education level) was performed to look at trends in data, and correlations between variables.

### Summary Tables
The full data set was summarized into several tables as part of the EDA. The tables below give key information about the data including:  
+ *Summary Count of Observations* for response Variable at each level: a measure to check if the data is balanced or not balanced for the response variable  
+ *Summary Statistics of Response Variable* with respect to different explanatory variables including *BMI*, *Mental_Health*, and *Physical_Health* of the survey respondent. These explanatory variables were chosen based on the fact that they are *numerical, and represent the general overall health of the respondent* The reader should look for the average value of explanatory variable for each setting of response variable  
+ Contingency tables: 3-way contingency table summarizing diabetes status based on demographic (age and sex) of the respondent. The reader should look for trends in diabetes status based on age and sex of the respondents.


```{r tables}
# Check for the balance of data 
summary(diabetes_education_level$Diabetes_binary)

# # Using the `group_by()` function, to find the summary BMI for 
# diabetes vs non-diabetes respondents in the survey
diabetes_education_level %>% group_by(Diabetes_binary) %>% 
  summarise(avg = mean(BMI), sd = sd(BMI),
            min = min(BMI), max = max(BMI))

# Summary Mental Health for diabetes vs non-diabetes respondents in the survey
diabetes_education_level %>% group_by(Diabetes_binary) %>% 
  summarise(avg = mean(MentHlth), sd = sd(MentHlth),
            min = min(MentHlth), max = max(MentHlth))

# Summary Physical Health for diabetes vs non-diabetes respondents in the survey
diabetes_education_level %>% group_by(Diabetes_binary) %>% 
  summarise(avg = mean(PhysHlth), sd = sd(PhysHlth),
            min = min(PhysHlth), max = max(PhysHlth))

# 3-way contingency table for diabetes status, age and sex
table(diabetes_education_level$Age, diabetes_education_level$Sex,
      diabetes_education_level$Diabetes_binary)

# contingency table 
table(diabetes_education_level$Diabetes_binary,
      diabetes_education_level$HighChol) 
# contingency table
table(diabetes_education_level$Diabetes_binary, 
      diabetes_education_level$Smoker)  

```



### Summary Plots
The full data set was summarized into several plots as part of the EDA.The plots below give key information about the data including:  
+ *Correlation of Numeric Variables*: The reader should look for any strong (positive or negative) correlation among the numeric variables to minimize collinearity  
+ *Barplot of respondents age and diabetes status*: The reader should look for trends in the age group of respondents and number of diabetic patients. A high number of diabetic patients in lower age groups might indicate an unhealthy life style or no access to health care or other issues which could be of public health concern
+ *Physical_Health* of the survey respondent. These explanatory variables were chosen based on the fact that they are *numerical, and represent the general overall health of the respondent* The reader should look for the average value of explanatory variable for each setting of response variable  
+ Contingency tables: 3-way contingency table summarizing diabetes status based on demographic (age and sex) of the respondent. The reader should look for trends in diabetes status based on age and sex of the respondents.



```{r}
# Correlation of numeric variables.
Correlation <- cor(select(diabetes_education_level, 
                          c("BMI","MentHlth","PhysHlth")))
corrplot(Correlation,  tl.pos = "lt")

# Barplot of respondents age and diabetes status
g1 <- ggplot(data = diabetes_education_level, aes(x = Age)) +
  labs(x = "Age Group", title = "Bar Plot of Age of Respondents in the diabetes study",
       y = "number of respondents") + 
  scale_fill_discrete(name = "Diabetes Status", labels = c("Have diabetes", "Don't have diabetes"))
g1 + geom_bar(aes(fill = Diabetes_binary)) + theme_bw()

# Scatter plot of 
g2 <- 
```


numeric_securities_data <- securities_data_wide %>% select(where(is.numeric)) 
Correlation <- cor(numeric_securities_data)

#A graphical summary of the correlation matrix was created using the corrplot library.


## Modeling

### Splitting Train and Test data sets

```{r}
set.seed(12345)
train <- sample(1:nrow(diabetes_education_level), size = nrow(diabetes_education_level)*0.7) # 70/30 split
test <- dplyr::setdiff(1:nrow(diabetes_education_level), train)
diabetes_train <- diabetes_education_level[train,]
daibetes_test <- diabetes_education_level[test,]
```

### Log-loss

Log-loss, also known as cross-entropy loss or logistic loss, is a commonly used metric in machine learning and specifically in classification problems. It measures the performance of a classification model whose output is a probability value between 0 and 1.

Log-loss is a more informative metric than accuracy for several reasons:

- It penalizes models for being confident in their wrong predictions. Accuracy simply counts the number of correct predictions, but it does not consider how confident the model was in those predictions. A model that is confident in its wrong predictions is more likely to make mistakes in the future. Log-loss penalizes models for being confident in their wrong predictions, which encourages them to be more cautious.

- It is robust to imbalanced datasets. When a dataset is imbalanced, one class may be much more common than the other class. This can lead to models that are biased towards predicting the majority class. Accuracy is not robust to imbalanced datasets, as it can be high even if the model is simply predicting the majority class all the time. Log-loss is robust to imbalanced datasets, as it penalizes models for being confident in their wrong predictions, even if they are predicting the majority class.

- It is more sensitive to small changes in model performance. Accuracy is a step function, meaning that it only changes when a model makes a mistake. Log-loss is a continuous function, meaning that it changes even when a model's predictions are slightly off. This makes log-loss a more sensitive metric for evaluating model performance, and it can be used to identify areas where a model can be improved.

In general, log-loss is a better metric than accuracy for evaluating the performance of machine learning models, particularly for classification problems. It is more informative, robust to imbalanced datasets, and more sensitive to small changes in model performance.


### Logistic Regression

Logistic regression is a statistical model that is used to predict the probability of a binary outcome. Logistic regression models are trained on historical data that includes both the outcome variable and a set of predictor variables. The model then learns to predict the probability of the outcome variable for new data points based on the values of the predictor variables.

Logistic regression models are a type of supervised learning model, which means that they are trained on labeled data. Labeled data is data where each data point has a known outcome value. This allows the model to learn the relationship between the predictor variables and the outcome variable.

We can use logistic regression on this project because the outcome of the dataset, Diabetes_binary, is in binary format.

```{r}

# Model 1: Forward Selection
Model_1_Forward <- train(
  Diabetes_binary ~ .-Education,
  data = diabetes_train,
  method = "glmStepAIC",
  family = "binomial",
  direction = "forward",
  metric = "logLoss",
  trace = FALSE,
  trControl = trainControl(method = "cv", 
                           number = 5,
                           preProcOptions = c("center","scale"),
                           summaryFunction = mnLogLoss,
                           classProbs = TRUE)
  )


# Model 2: Backward Selection
Model_2_Backward <- train(
  Diabetes_binary ~ . -Education,
  data = diabetes_train,
  method = "glmStepAIC",
  family = "binomial",
  direction = "backward",
  metric = "logLoss",
  trace = FALSE,
  trControl = trainControl(method = "cv", 
                           number = 5,
                           preProcOptions = c("center","scale"),
                           summaryFunction = mnLogLoss,
                           classProbs = TRUE)
  )


# Model 3: Full model
Model_3_Full <- train(
  Diabetes_binary ~ . -Education,
  data = diabetes_train,
  method = "glm",
  metric = "logLoss",
  trace = FALSE,
  trControl = trainControl(method = "cv", 
                           number = 5,
                           preProcOptions = c("center","scale"),
                           summaryFunction = mnLogLoss,
                           classProbs = TRUE)
  )

Model_1_Forward
Model_2_Backward
Model_3_Full

```
The Logloss value of Forward selection model is `Model_1_Forward$results$logLoss`, the Logloss value of Backward selection model is `Model_2_Backward$results$logLoss`, and the Logloss value of Full model is `Model_3_Full$results$logLoss`. The second model has the lowest Logloss value, so it is the "best" model in these three.  


### LASSO Logistic Regression Model

```{r}

```


### Classification Tree Model

```{r}

```


### Random Forest Model

A random forest is a machine learning algorithm that uses a collection of decision trees to make predictions. Decision trees are a type of supervised learning algorithm that can be used for classification and regression tasks. They work by building a tree-like structure that splits the data into different branches based on the values of the features. The leaves of the tree represent the predictions.

Random forests differ from decision trees in two main ways:

 - Random forests use multiple decision trees. This is called ensemble learning. By averaging the predictions of multiple decision trees, random forests can reduce the risk of overfitting and improve the overall accuracy of the model.
 
 - Random forests use a random subset of features to build each decision tree. This is called feature bagging. Feature bagging helps to reduce the correlation between the decision trees, which further improves the accuracy of the model.

```{r}

# Random Forest Model
Model_rf <- train(
  Diabetes_binary ~ . -Education,
  data = diabetes_train,
  method = "rf",
  metric = "logLoss",
  trControl = trainControl(method = "cv", 
                           number = 5,
                           preProcOptions = c("center","scale"),
                           summaryFunction = mnLogLoss,
                           classProbs = TRUE),
  tuneGrid = data.frame(mtry = seq(1:10))
  )

```


### Partial Least Squares Model

Partial Least Squares (PLS) is a statistical method that is often used in regression and classification analysis. PLS regression is commonly employed when there are high collinearity among the predictor variables, and it aims to find the directions (latent variables or components) that explain both the variance in the predictor variables and the variance in the response variable.

Partial Least Squares Discriminant Analysis (PLS-DA) is an extension of PLS that is specifically used for classification purposes. PLS-DA combines elements of principal component analysis (PCA) and canonical correlation analysis to find the linear combinations of the original variables (features) that best discriminate between different classes in the response variable.

In the context of classification, the goal of PLS-DA is to find a set of latent variables (components) that maximize the separation between different classes while also explaining the variance in the predictor variables. These latent variables are then used to build a predictive model for classifying new observations into predefined classes.

PLS and PLS-DA are commonly used in fields such as chemometrics, biology, and other areas where there are complex relationships between variables and a need for effective classification or regression models. They are particularly useful when dealing with high-dimensional data or when there are multicollinearity issues among the predictor variables.

```{r}

# Partial Least Squares Model
Model_pls <- train(
  Diabetes_binary ~ . -Education,
  data = diabetes_train,
  method = "pls",
  metric = "logLoss",
  trControl = trainControl(method = "cv", 
                           number = 5,
                           preProcOptions = c("center","scale"),
                           summaryFunction = mnLogLoss,
                           classProbs = TRUE),
  tuneGrid = data.frame(ncomp = seq(1:15))
  )

```


### Another Model

```{r}

```


## Final Model Selection

```{r}

```

