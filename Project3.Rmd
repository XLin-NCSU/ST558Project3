---
title: "Project 3"
output: github_document
author: Xi Lin, Sarat Bantupalli
params: 
  Education: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### This report is for Education level `r params$Education`.

## Introduction



## Load packages

```{r load packages, echo=FALSE}
library(tidyverse)
library(caret)
```

## Data processing

```{r}
diabetes <- read.csv("diabetes_binary_health_indicators_BRFSS2015.csv")

diabetes <- diabetes %>% mutate(
  Education = if_else(Education == 1, 2, Education) %>% as.factor()
) # merge group Edu=1 into group Edu=2

diabetes$Diabetes_binary <- if_else(diabetes$Diabetes_binary == 0, "No_Diabete","Diabete")

columns_to_factor <- c("Diabetes_binary",
                       "HighBP",
                       "HighChol",
                       "CholCheck",
                       "Smoker",
                       "Stroke",
                       "HeartDiseaseorAttack",
                       "PhysActivity",
                       "Fruits",
                       "Veggies",
                       "HvyAlcoholConsump",
                       "AnyHealthcare",
                       "NoDocbcCost",
                       "GenHlth",
                       "DiffWalk",
                       "Sex",
                       "Age",
                       "Income")


diabetes[, columns_to_factor] <- lapply(diabetes[, columns_to_factor], factor) # change the column type to factor

Diabetes_Edu <- subset(diabetes, Education == params$Education)
```

## EDA and Summarizations

```{r}

summary(Diabetes_Edu$Diabetes_binary) # check the response variable

plot(select(Diabetes_Edu, c("BMI","MentHlth","PhysHlth"))) # check the correlation of numeric variables.

table(Diabetes_Edu$Diabetes_binary,Diabetes_Edu$HighChol) # contingency table 

table(Diabetes_Edu$Diabetes_binary, Diabetes_Edu$Smoker) # contingency table 

```


## Modeling

### Splitting Train and Test data sets

```{r}
set.seed(12345)
train <- sample(1:nrow(Diabetes_Edu), size = nrow(Diabetes_Edu)*0.7) # 70/30 split
test <- dplyr::setdiff(1:nrow(Diabetes_Edu), train)
diabetes_train <- Diabetes_Edu[train,]
daibetes_test <- Diabetes_Edu[test,]
```

### Log-loss

Log-loss, also known as cross-entropy loss or logistic loss, is a commonly used metric in machine learning and specifically in classification problems. It measures the performance of a classification model whose output is a probability value between 0 and 1.

Log-loss is a more informative metric than accuracy for several reasons:

- It penalizes models for being confident in their wrong predictions. Accuracy simply counts the number of correct predictions, but it does not consider how confident the model was in those predictions. A model that is confident in its wrong predictions is more likely to make mistakes in the future. Log-loss penalizes models for being confident in their wrong predictions, which encourages them to be more cautious.

- It is robust to imbalanced datasets. When a dataset is imbalanced, one class may be much more common than the other class. This can lead to models that are biased towards predicting the majority class. Accuracy is not robust to imbalanced datasets, as it can be high even if the model is simply predicting the majority class all the time. Log-loss is robust to imbalanced datasets, as it penalizes models for being confident in their wrong predictions, even if they are predicting the majority class.

- It is more sensitive to small changes in model performance. Accuracy is a step function, meaning that it only changes when a model makes a mistake. Log-loss is a continuous function, meaning that it changes even when a model's predictions are slightly off. This makes log-loss a more sensitive metric for evaluating model performance, and it can be used to identify areas where a model can be improved.

In general, log-loss is a better metric than accuracy for evaluating the performance of machine learning models, particularly for classification problems. It is more informative, robust to imbalanced datasets, and more sensitive to small changes in model performance.


### Logistic Regression

Logistic regression is a statistical model that is used to predict the probability of a binary outcome. Logistic regression models are trained on historical data that includes both the outcome variable and a set of predictor variables. The model then learns to predict the probability of the outcome variable for new data points based on the values of the predictor variables.

Logistic regression models are a type of supervised learning model, which means that they are trained on labeled data. Labeled data is data where each data point has a known outcome value. This allows the model to learn the relationship between the predictor variables and the outcome variable.

We can use logistic regression on this project because the outcome of the dataset, Diabetes_binary, is in binary format.

```{r}

# Model 1: Forward Selection
Model_1_Forward <- train(
  Diabetes_binary ~ .-Education,
  data = diabetes_train,
  method = "glmStepAIC",
  family = "binomial",
  direction = "forward",
  metric = "logLoss",
  trace = FALSE,
  trControl = trainControl(method = "cv", 
                           number = 5,
                           preProcOptions = c("center","scale"),
                           summaryFunction = mnLogLoss,
                           classProbs = TRUE)
  )


# Model 2: Backward Selection
Model_2_Backward <- train(
  Diabetes_binary ~ . -Education,
  data = diabetes_train,
  method = "glmStepAIC",
  family = "binomial",
  direction = "backward",
  metric = "logLoss",
  trace = FALSE,
  trControl = trainControl(method = "cv", 
                           number = 5,
                           preProcOptions = c("center","scale"),
                           summaryFunction = mnLogLoss,
                           classProbs = TRUE)
  )


# Model 3: Full model
Model_3_Full <- train(
  Diabetes_binary ~ . -Education,
  data = diabetes_train,
  method = "glm",
  metric = "logLoss",
  trace = FALSE,
  trControl = trainControl(method = "cv", 
                           number = 5,
                           preProcOptions = c("center","scale"),
                           summaryFunction = mnLogLoss,
                           classProbs = TRUE)
  )

Model_1_Forward
Model_2_Backward
Model_3_Full

```
The Logloss value of Forward selection model is `Model_1_Forward$results$logLoss`, the Logloss value of Backward selection model is `Model_2_Backward$results$logLoss`, and the Logloss value of Full model is `Model_3_Full$results$logLoss`. The second model has the lowest Logloss value, so it is the "best" model in these three.  


### LASSO Logistic Regression Model

```{r}

```


### Classification Tree Model

```{r}

```


### Random Forest Model

A random forest is a machine learning algorithm that uses a collection of decision trees to make predictions. Decision trees are a type of supervised learning algorithm that can be used for classification and regression tasks. They work by building a tree-like structure that splits the data into different branches based on the values of the features. The leaves of the tree represent the predictions.

Random forests differ from decision trees in two main ways:

 - Random forests use multiple decision trees. This is called ensemble learning. By averaging the predictions of multiple decision trees, random forests can reduce the risk of overfitting and improve the overall accuracy of the model.
 
 - Random forests use a random subset of features to build each decision tree. This is called feature bagging. Feature bagging helps to reduce the correlation between the decision trees, which further improves the accuracy of the model.

```{r}

# Random Forest Model
Model_rf <- train(
  Diabetes_binary ~ . -Education,
  data = diabetes_train,
  method = "rf",
  metric = "logLoss",
  trControl = trainControl(method = "cv", 
                           number = 5,
                           preProcOptions = c("center","scale"),
                           summaryFunction = mnLogLoss,
                           classProbs = TRUE),
  tuneGrid = data.frame(mtry = seq(1:10))
  )

```


### Partial Least Squares Model

Partial Least Squares (PLS) is a statistical method that is often used in regression and classification analysis. PLS regression is commonly employed when there are high collinearity among the predictor variables, and it aims to find the directions (latent variables or components) that explain both the variance in the predictor variables and the variance in the response variable.

Partial Least Squares Discriminant Analysis (PLS-DA) is an extension of PLS that is specifically used for classification purposes. PLS-DA combines elements of principal component analysis (PCA) and canonical correlation analysis to find the linear combinations of the original variables (features) that best discriminate between different classes in the response variable.

In the context of classification, the goal of PLS-DA is to find a set of latent variables (components) that maximize the separation between different classes while also explaining the variance in the predictor variables. These latent variables are then used to build a predictive model for classifying new observations into predefined classes.

PLS and PLS-DA are commonly used in fields such as chemometrics, biology, and other areas where there are complex relationships between variables and a need for effective classification or regression models. They are particularly useful when dealing with high-dimensional data or when there are multicollinearity issues among the predictor variables.

```{r}

# Partial Least Squares Model
Model_pls <- train(
  Diabetes_binary ~ . -Education,
  data = diabetes_train,
  method = "pls",
  metric = "logLoss",
  trControl = trainControl(method = "cv", 
                           number = 5,
                           preProcOptions = c("center","scale"),
                           summaryFunction = mnLogLoss,
                           classProbs = TRUE),
  tuneGrid = data.frame(ncomp = seq(1:15))
  )

```


### Another Model

```{r}

```


## Final Model Selection

```{r}

```

