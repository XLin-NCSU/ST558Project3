---
title: "Project 3"
#output: github_document
output: pdf_document
author: Xi Lin, Sarat Bantupalli
#params: 
  #Education: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, message = FALSE, warning = FALSE)
```

### This report is for Education level `r params$Education`.

## Required Packages

```{r packages, echo=FALSE}
library(tidyverse)
library(corrplot)
library(caret)
library(MLmetrics)

```


## Introduction
This project creates predictive models using available data and automates R Markdown reports. We demonstrated it by using [Diabates Health Indicators Data](https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset) collected by the Centers of Disease Control and Prevention (CDC).  

The data was collected through a telephonic survey by the CDC in the year 2015 and it corresponds to the prevalence of diabetes in the survey respondents. The data set we analyzed here represents responses from 253,680 Americans on health-related risk behaviors, chronic health conditions, and the use of preventative services. The data set itself is *not balanced*.  

### Model Variables
The response variable of interest from the data set is *Diabetes_binary*. It represents if the survey respondent is *non diabetic (value 0)* or *pre-diabetic/ diabetic (value 1)*.

The explanatory variables were chosen such that important risk factors for diabetes including health & lifestyle, socioeconomic status, and demographics (age and sex) are considered in the predictive model. We included the following explanatory variables for the model:  

+ *HighBP*: This variable represents if a respondent was diagnosed with high blood pressure (value 1) or not (value 0). Research indicates high blood pressure is twice as likely to strike a person with diabetes than a person without diabetes  
+ *HighChol*: This variable represents if a respondent has high cholesterol (value 1) or not (value 0). Research suggests people with diabetes tend to have higher cholesterol levels  
+ *BMI*: The body mass index, a numerical variable, is a good indicator for obesity and represents the overall health of a person making it an ideal candidate for the model  
+ *Smoker*: Smoking increases the risk of diabetes and hence was included in the model. This variable can take two values, 0- did not smoke 100 cigarettes in lifetime or 1- smoked more than 100 cigarettes in lifetime  
+ *PhysicalActivity*: This variable indicates if the respondent had been physically active in the past 30 days. A value 0 represents no while 1 represents yes  
+ *Fruits*: A healthy diet is key to reducing the risk of type 2 diabetes and can represent lifestyle and socioeconomic status of the respondent. This variable indicates if the person is eating healthy or not. A value of 0 indicates no fruit consumed per day while 1 indicates some fruit consumed per day  
+ *Veggies*: This variable indicates if the person is eating healthy or not. It can represent lifestyle and socioeconomic status of the respondent. A value of 0 indicates no veggies consumed per day while 1 indicates some veggies consumed per day  
+ *HvyAlcoholConsump*: Alcohol increases the risk of diabetes and this variable encapsulates if the respondent is a heavy drinker  
+ *AnyHealthcare*: This variable represents socioeconomic status of the respondent, with low-income respondents not having any health coverage (value 0) while higher income respondents having some type of health coverage (value 1)  
+ *GenHlth*: This represents the overall health of the respondent ranging from 1 through 5 with 1 being the best health indicator  
+ *MentHlth*: This numerical variable represents days of poor mental health (1-30 days). People with mental health are 2 to 3 times more likely to have a depression than people without diabetes. This variable captures the overall health of the respondent  
+ *PhysHlth*: This numerical variable indicates physical illness or injury days in the past 30 days. Illness or injury can impact physical activity which in turn could affect diabetes risk  
+ *Sex*: Including the sex of the respondent (0- female, 1- male) could help us see if risk factors are different for men and women  
+ *Age*: Risk for type 2 diabetes increases with age. This 13 level categorical variable encapsulates this risk  
+ *Education*: Education might effect the socioeconomic status of a person and change the risk factor for diabetes. This variable was used to automate the R Markdown reports. Each report represents analysis for each level of this variable. A value of 2 indicates the respondent attended elementary school or less while 6 represents a college graduate  
+ *Income*: This categorical variable can affect factors that influence the risk for type 2 diabetes including access to health care, physical activity (eg. gym), and a healthy lifestyle. A value of 1 indicates low income (earning less than \$10,000), and a value of 8 indicates high income (>$75,000)  

### Purpose of EDA and Modeling
We used this project to develop predictive models for diabetes risk. With over 110 million Americans who are either diabetic or pre-diabetic, predictive models such as this can be of immense help for public health officials. We looked at how health & lifestyle, socioeconomic status, and demographics can help us predict the risk of diabetes for a person living in the United States. We have used the power of R Markdown to automate the report generation process based on the *Education* level of the person.  

Exploratory data analysis was the first step of this predictive modeling project. Although the variable selection process was based on knowledge of the field and metadata of the data set, EDA played an important role. EDA helped us visualize the data, looks for trends, and see correlation between variables.  


## Data Processing
The data for the analysis was read in using the `read_csv` function from the `readr` package and stored in the R object, *diabetes_data*. Then for the variable *Education*, groups 1 and 2 were combined into *group 2*. This was done to have five distinct groups for this variable ranging from 2 through 6.  

Then the response variable and some of the explanatory variables we are interested in were converted to factors. Explanatory variables that were converted to factors with meaningful level names include *HighBP*, *HighChol*, *Smoker*, *PhysicalActivity*, *Fruits*, *Veggies*, *HvyAlcoholConsump*, *AnyHealthcare*, *GenHlth*, *Sex*, *Age*, and *Income*.  

The data was then subset to represent different *Education* levels of the respondents in the survey.

```{r data_processing}
#Read in data using the read_csv function from readr package
diabetes_data <- read_csv("diabetes_binary_health_indicators_BRFSS2015.csv")

#Combine groups 1 and 2 into group 2 for Education variable
diabetes_data <- diabetes_data %>% 
  mutate(Education = if_else(Education == 1, 2, Education) %>% as.factor())

# Re-name the Diabetes_binary variable to a more meaningful description
diabetes_data$Diabetes_binary <- if_else(diabetes_data$Diabetes_binary == 0, 
                                    "No_Diabetes","Diabetes")

# Select only the variables we are interested in
diabetes_data <- diabetes_data %>% 
  select(Diabetes_binary, HighBP, HighChol, BMI, Smoker, PhysActivity,
         Fruits, Veggies, HvyAlcoholConsump, AnyHealthcare, GenHlth,
         MentHlth, PhysHlth, Sex, Age, Education, Income)

# Create a vector with the columns we want to convert to factors
columns_to_factor <- c("Diabetes_binary",
                       "HighBP",
                       "HighChol",
                       "Smoker",
                       "PhysActivity",
                       "Fruits",
                       "Veggies",
                       "HvyAlcoholConsump",
                       "AnyHealthcare",
                       "GenHlth",
                       "Sex",
                       "Age",
                       "Income")

# Change the columns we are interested in to a factor using the lapply function
diabetes_data[, columns_to_factor] <- lapply(diabetes_data[, columns_to_factor], factor)

# Subset the data to work on the Education level of interest
#diabetes_education_level <- subset(diabetes_data, Education == params$Education) %>% 
  #select(-Education)

diabetes_education_level <- subset(diabetes_data, Education == 2) %>% 
  select(-Education)
```

## Summarizations and Exploratory Data Analysis (EDA)
Here Exploratory Data Analysis on the full data (for the particular education level) was performed to look at trends in data, and correlations between variables.

### Summary Tables
The full data set was summarized into several tables as part of the EDA. The tables give summarize the information in the data. We looked at how health, socioeconomic status, and demographics affect diabetes_status.  

Using the `summary` function to look at the data points for Education level `r params$Education`.
```{r}
# Check for the balance of data 
summary(diabetes_education_level$Diabetes_binary)
```
The reader should notice if the data is balanced or not balanced for the response variable.  

Here the summary statistics of diabetes_status with respect to *BMI* were found using the `group_by()` function. BMI is a good indicator for obesity and represents the overall health of a person.

```{r}
# Summary BMI for diabetes vs non-diabetes respondents in the survey
diabetes_education_level %>% group_by(Diabetes_binary) %>% 
  summarise(avg = mean(BMI), sd = sd(BMI),
            min = min(BMI), max = max(BMI))

```

The reader should look for the average value of *BMI* for each setting of diabetes_status. On average, a lower value of BMI is expected for people without diabetes when compared to people with diabetes.  

Here the summary statistics of diabetes_status with respect to *Mental Health* were found. It captures the overall health of a person.

```{r}
# Summary Mental Health for diabetes vs non-diabetes respondents in the survey
diabetes_education_level %>% group_by(Diabetes_binary) %>% 
  summarise(avg = mean(MentHlth), sd = sd(MentHlth),
            min = min(MentHlth), max = max(MentHlth))

```
A low number of poor mental health days on average is expected for people with no diabetes while the contrary for people with diabetes.  

Here the summary statistics of diabetes_status with respect to *Physical Health* were found. It captures the health aspect of a person.
Illness or injury can impact physical activity which in turn could affect diabetes risk.

```{r}
# Summary Physical Health for diabetes vs non-diabetes respondents in the survey
diabetes_education_level %>% group_by(Diabetes_binary) %>% 
  summarise(avg = mean(PhysHlth), sd = sd(PhysHlth),
            min = min(PhysHlth), max = max(PhysHlth))
```
People without diabetes are expected on average to be more physically active. A low average number of days with psychical injury is expected for people without diabetes when compared to people with diabetes.  

Socioeconomic status plays an important role in risk for diabetes. Here we summarized the impact of *Income* on diabetes_status.

```{r}
diabetes_education_level %>% group_by(Diabetes_binary) %>% 
  count(Income)

```
The reader should look for trends on how Income of the respondent plays a role in diabetes_status.  


Here we looked at demographics impact (age and sex) on diabetes_status using a 3-way contingency table.

```{r}
# 3-way contingency table for diabetes status, age and sex
table(diabetes_education_level$Age, diabetes_education_level$Sex,
      diabetes_education_level$Diabetes_binary)
```
The reader should look for differences in male and female diabetes_status and also how age impacts it!!  


### Summary Plots
The full data set was summarized into several plots as part of the EDA.The plots below give key information about trends in data. Using graphics, we looked at how health, socioeconomic status, and demographics affect diabetes_status.  

Here, we created a visual of the correlation among different health indicating variables using the *corrplot* library. 
```{r}
# Correlation of numeric variables.
Correlation <- cor(select(diabetes_education_level, 
                          c("BMI","MentHlth","PhysHlth")))
corrplot(Correlation,  tl.pos = "lt")
```

The reader should look for any strong (positive or negative) correlation among the numeric variables to minimize collinearity.  

Here we visualized trends in diabetes_status for people with different health risks: poor health (have high blood pressure, high cholesterol, and is a smoker) and good health (no high blood pressure or cholesterol, not a smoker).

```{r}
diabetes_education_level %>% group_by(Smoker, HvyAlcoholConsump) %>% 
  ggplot(aes(x = Diabetes_binary)) + geom_bar(aes(fill = Smoker))


```



Demographics might have a role in risk for diabetes. Here we looked at trends on how age impacts diabetes_status. 

```{r}
# Barplot of respondents age and diabetes status
g1 <- ggplot(data = diabetes_education_level, aes(x = Age)) +
  labs(x = "Age Group", title = "Bar Plot of Age of Respondents in the diabetes study",
       y = "number of respondents") + 
  scale_fill_discrete(name = "Diabetes Status", labels = c("Have diabetes", "Don't have diabetes"))
g1 + geom_bar(aes(fill = Diabetes_binary)) + theme_bw()

```

The reader should look for trends in how proportion of people with diabetes changes with age.  

As part of demographics, we looked at trends in diabetes_status for men and women here.

```{r}
# Barplot of respondents age and diabetes status
g2 <- ggplot(data = diabetes_education_level, aes(x = Sex)) +
  labs(x = "Sex", title = "Bar Plot of Sex of Respondents in the diabetes study",
       y = "number of respondents") + 
  scale_fill_discrete(name = "Diabetes Status", labels = c("Have diabetes", "Don't have diabetes")) +
  scale_x_discrete(labels = c("Female", "Male"))
g2 + geom_bar(aes(fill = Diabetes_binary)) + theme_bw()

```
The reader should look for trends in how diabetes_status changes for men and women. 


## Modeling

### Splitting Train and Test data sets
Here the data was split into training set (70% of data) and test set (30% of data). We have used the `set.seed` function to have reproducible results.

```{r}
set.seed(12345)
# Create training data using createDataPartition function from caret package

train_data_index <- createDataPartition(diabetes_education_level$Diabetes_binary, p = 0.7, 
                                        list = FALSE)
diabetes_train <- diabetes_education_level[train_data_index, ]
diabetes_test <- diabetes_education_level[-train_data_index, ]

```

### Log-loss

Log-loss, also known as cross-entropy loss or logistic loss, is a *common evaluation metric* for binary classification problems. It is indicative of how close the prediction probability is to the actual value. A *lower log-loss* value indicates a lower deviance in predicted probability vs observed probability and is desired.  

Log-loss is a more informative metric than other metrics like accuracy for several reasons:  

+ It **penalizes models** for being confident in their wrong predictions. Accuracy simply counts the number of correct predictions, but it does not consider how confident the model was in those predictions. A model that is confident in its wrong predictions is more likely to make mistakes in the future. Log-loss penalizes models for being confident in their wrong predictions, which encourages them to be more cautious.  
+ It is **robust to imbalanced data sets**. When a data set is imbalanced (which is the case here), one class may be much more common than the other class. This can lead to models that are biased towards predicting the majority class. Accuracy is not robust to imbalanced data sets, as it can be high even if the model is simply predicting the majority class all the time. Log-loss is robust to imbalanced data sets, as it penalizes models for being confident in their wrong predictions, even if they are predicting the majority class.  
+ It is more **sensitive to small changes in model performance**. Accuracy is a step function, meaning that it only changes when a model makes a mistake. Log-loss is a continuous function, meaning that it changes even when a model's predictions are slightly off. This makes log-loss a more sensitive metric for evaluating model performance, and it can be used to identify areas where a model can be improved.  

In general, log-loss is a better metric than accuracy for evaluating the performance of models. Since the data set we are working on for this project is imbalanced, it is ideal to use Log-loss as the model metric when compared to accuracy. 


### Logistic Regression

Generalized linear models (GLM) are used to model non-normal distributions. Logistic regression is a very common generalized linear model that is used to predict the probability of a binary outcome (success or fail). This type of model is used for both classification and regression. Since the outcome is a probability, the response variable is constrained to values between 0 and 1.   

Here the probability of success is modeled using a function that does not have a closed form solution. Hence, maximum likelihood is often used to fit the parameters. In a logistic model, a log transformation of this function is performed, usually called log-odds or the *logit function*. The logit function is the *link* that linearly associates mean of response variable to the parameters in model.  

For the current data set, our variable of interest, Diabetes_binary, is binomial random variable i.e. it has a value of either 0 or 1. Logistic regression would be an ideal model to apply here. 

#### Model 1
Here we used the *forward selection method* to construct a regression model. We trained the model with all variables from the training data set. Only main effects were considered here. Interaction terms and higher polynomial terms were not considered. The model was with the following parameters:  
+ cross-validation with 5 folds  
+ log-loss as metric to evaluate model

```{r forward_selection}
# Model 1: Forward Selection
model_1_forward <- train( Diabetes_binary ~ .,
                          data = diabetes_train,
                          method = "glmStepAIC",
                          family = "binomial",
                          direction = "forward",
                          metric = "logLoss",
                          trace = FALSE,
                          preProcess = c("center", "scale"),
                          trControl = 
                            trainControl(method = "cv", number = 5, 
                                         summaryFunction = mnLogLoss, 
                                         classProbs = TRUE))

model_1_forward
```

The Logloss value of Forward selection model is `{r} model_1_forward$results$logLoss`.

#### Model 2
Here we used the *backward selection method* to construct a regression model. We trained the model with all variables from the training data set. Only main effects were considered here. Interaction terms and higher polynomial terms were not considered. The model was with the following parameters:  
+ cross-validation with 5 folds  
+ log-loss as metric to evaluate model

```{r backward_selection}
# Model 2: Backward Selection
model_2_backward <- train( Diabetes_binary ~ .,
                           data = diabetes_train,
                           method = "glmStepAIC",
                           family = "binomial",
                           direction = "backward",
                           metric = "logLoss",
                           trace = FALSE,
                           preProcess = c("center", "scale"),
                           trControl = 
                             trainControl(method = "cv", number = 5,
                                          summaryFunction = mnLogLoss, 
                                          classProbs = TRUE))

model_2_backward
```

The Logloss value of Backward selection model is `{r} model_2_backward$results$logLoss`.

#### Model 3
Here we fit a full logistic regression model. We trained the model with all variables from the training data set. Only main effects were considered here. Interaction terms and higher polynomial terms were not considered. The model was with the following parameters:  
+ cross-validation with 5 folds  
+ log-loss as metric to evaluate model


```{r glm}
# Model 3: Generalized Linear Model
model_3_glm <- train( Diabetes_binary ~ ., 
                       data = diabetes_train,
                       method = "glm",
                       metric = "logLoss",
                       trace = FALSE,
                       preProcess = c("center", "scale"),
                       trControl = 
                         trainControl(method = "cv", number = 5,
                                      summaryFunction = mnLogLoss,
                                      classProbs = TRUE))

model_3_glm
```

The Logloss value of Backward selection model is `{r} model_3_glm$results$logLoss`.

#### Comparison of Models
The logloss values for all 3 models is tabulated below.

```{r}
result1 <- data.frame(Model_1 = model_1_forward$results$logLoss, 
            Model_2 = model_2_backward$results$logLoss,
            Model_3 = model_3_glm$results$logLoss)

result1[1,]
```

The lowest logloss value is for `{r} names(result1)[apply(result1, MARGIN = 1, FUN = which.min)]` with a value of `{r} min(result1)`. We choose **`{r} names(result1)[apply(result1, MARGIN = 1, FUN = which.min)]`** as the best model.


### LASSO Logistic Regression Model

LASSO or Least Absolute Shrinkage and Selection Operator, is a regression analysis model that *performs both variable selection and regularization* to improve prediction accuracy. It is a **penalized** regression approach that estimates the regression coefficients by minimizing the sum of squared residuals and the sum of the absolute values of the regression coefficients multiplied with a positive constant, $\lambda$. Mathematically it can be expressed as:  

$$
L +\lambda \sum_{i = 1}^{n} |\beta_i|

$$
where L is the sum of squared residuals, and $\lambda \sum_{i = 1}^{n} |\beta_i|$ is the penalty.  

+ when $\lambda$ = 0, all variables are included  
+ when $\lambda$ = $\infty$, no variables are chosen  
+ usually cross-validation is used to choose the value of $\lambda$ such that some coefficients will shrink to 0  

A LASSO Model uses a **L1 Regularization technique**. Regularization refers to techniques that are used to calibrate models in order to minimize modified sum of squared errors (above equation) and prevent over fitting or under fitting data. 

In a linear model like basic logistic regression (essentially $\lambda$ = 0 or no penalty), they tend to have some variance, that is the model does not generalize well to data other than training set. In the case of a LASSO model, regularization significantly reduces variance of the model without an increase in bias, a balance of bias-variance trade off. This is achieved with the help of the tuning parameter, $\lambda$. As $\lambda$ increases, it reduces the value of coefficients and thus reducing the variance of the model, avoiding an over fit. But after certain increase in $\lambda$, we loose important features of the data and this will increase the bias resulting in under fitting. Hence, an optimal value of $\lambda$ is chosen using cross-validation.  

Here a LASSO logistic regression model was fit with the following parameters:  
+ $\alpha$ = 1  
+ $\lambda$ : a sequence of numbers from 0 to 1 with an increment of 0.01  
+ cross-validation with 5 folds  
+ log-loss as metric to evaluate model

```{r lasso_fit}

lasso_fit <- train(Diabetes_binary  ~ ., 
              data = diabetes_train,
              method = "glmnet",
              metric = "logLoss",
              preProcess = c("center", "scale"),
              trControl = trainControl(method = "cv", number = 5, 
                                       summaryFunction = mnLogLoss, classProbs = TRUE),
              tuneGrid = expand.grid(alpha = 1, lambda=seq(0,1 , by = 0.01)))

lasso_fit
```

The plot below shows the model performance with different $\lambda$ values.

```{r}
plot(lasso_fit)

```

The best LASSO model has an optimal $\lambda$ value of `{r} lasso_fit$bestTune$lambda` with a logLoss of `{r} min(lasso_fit$results$logLoss)`.


### Classification Tree Model

Tree based method is a *non-linear* regression model used for prediction. In a tree based method, the predictor space is *split into regions*, and each region has a different prediction. For a given region, the most prevalent class is used as the prediction in a classification problem. While the mean of observations in the region is used as prediction in the case of a continuous variable.  

Tree based methods use a greedy algorithm called **recursive binary splitting** to pick the splits. For every possible value of each predictor, they find the residual sum of squares (RSS) or Gini index/Deviance (for classification) and try to minimize it. The split it made at the location where RSS is split. Then the algorithm repeats the process till we have a large tree with several nodes. This tree is then pruned back using **cost-complexity pruning** so that we do not over fit the model to training data. The pruning process increases the bias but decreases the variance, finding a balance of bias-variance trade off. Cross-validation can help choosing the optimum number of nodes in a tree.  

Tree based methods are flexible, very intuitive and easy to read. They also do not need interactions terms in the model. This makes them an ideal candidate to use here.  

Here a classification tree model was fit with the following parameters:  
+ complexity parameter : a sequence of numbers from 0 to 0.1 with an increment of 0.001  
+ cross-validation with 5 folds  
+ log-loss as metric to evaluate model

```{r classification_tree}
classification_tree <- train(Diabetes_binary ~ ., 
                 data = diabetes_train,
                 method = "rpart",
                 metric = "logLoss",
                 trControl = trainControl(method = "cv", number = 5, 
                                          summaryFunction = mnLogLoss, classProbs = TRUE),
                 tuneGrid = expand.grid(cp = seq(from = 0, to = 0.1, by = 0.001)))

classification_tree
```

The plot below shows the model performance with different complexity parameter values.

```{r}
 plot(classification_tree)

```

The best classification tree model has an optimal complexity parameter value of `{r} classification_tree$bestTune$cp` with a logLoss of `{r} min( classification_tree$results$logLoss)`.

### Random Forest Model

Random forest is a supervised non-linear machine learning algorithm that uses a collection of decision trees from bootstrap samples and averages the results to make predictions. Rather than using all the predictors for the trees, it takes a randomly selected subset of variables for each bootstrap sample/ tree fit. By using only a subset of randomly chosen predictors, the random forest model reduces the number of correlated trees and hence an overall reduction in variance occurs.  

Random forests differ from decision trees in two main ways:  

 + Random forests use multiple decision trees. This is called ensemble learning. By averaging the predictions of multiple decision trees, random forests can reduce the risk of over fitting and improve the overall accuracy of the model.  
 + Random forests use a random subset of features to build each decision tree. This is called feature bagging. Feature bagging helps to reduce the correlation between the decision trees, which further improves the accuracy of the model.  
 
Here the random forest method was used to fit the data with the following parameters:  
+ tuning parameter, mtry: 1 through 5 with an increment of 1. This number was arrived by dividing number of predictors by 3  
+ cross-validation with 5 folds  
+ log-loss as metric to evaluate model

```{r random_forest}
# Random Forest Model
rf_model <- train( Diabetes_binary ~ .,
                   data = diabetes_train,
                   method = "rf",
                   metric = "logLoss",
                   trControl = 
                     trainControl(method = "cv", number = 5,
                                  summaryFunction = mnLogLoss, classProbs = TRUE),
                   tuneGrid = data.frame(mtry = seq(1:5)))

```

The plot below shows the model performance with different number of randomly selected predictors.
```{r}
plot(rf_model)

```

The best random forest model has `{r} rf_model$bestTune$mtry` randomly selected predictors with a logLoss of `{r} min( rf_model$results$logLoss)`.


### Partial Least Squares Model

Partial Least Squares (PLS) is a statistical method that is often used in regression and classification analysis. PLS regression is commonly employed when there are high collinearity among the predictor variables, and it aims to find the directions (latent variables or components) that explain both the variance in the predictor variables and the variance in the response variable.  

Partial Least Squares Discriminant Analysis (PLS-DA) is an extension of PLS that is specifically used for classification purposes. PLS-DA combines elements of principal component analysis (PCA) and canonical correlation analysis to find the linear combinations of the original variables (features) that best discriminate between different classes in the response variable.  

In the context of classification, the goal of PLS-DA is to find a set of latent variables (components) that maximize the separation between different classes while also explaining the variance in the predictor variables. These latent variables are then used to build a predictive model for classifying new observations into predefined classes.  

PLS and PLS-DA are commonly used in fields such as chemometrics, biology, and other areas where there are complex relationships between variables and a need for effective classification or regression models. They are particularly useful when dealing with high-dimensional data or when there are multicollinearity issues among the predictor variables.  

Here the pls method was used to fit the data with the following parameters:  
+ ncomp set to values 0 through 15  
+ cross-validation with 5 folds  
+ log-loss as metric to evaluate model

```{r pls}
# Partial Least Squares Model
pls_model <- train( Diabetes_binary ~ .,
                    data = diabetes_train,
                    method = "pls",
                    metric = "logLoss",
                    trControl = trainControl(method = "cv", number = 5,
                                             preProcOptions = c("center","scale"),
                                             summaryFunction = mnLogLoss,
                                             classProbs = TRUE),
                    tuneGrid = data.frame(ncomp = seq(1:15)))

pls_model
```

The plot below shows the model performance with different ncomp values.
```{r}
plot(pls_model)

```

The best pls model has `{r} pls_model$bestTune$ncomp` number of components in the model with a logLoss of `{r} min(pls_model$results$logLoss)`.


### Naive Bayes Model

Naive Bayes model, also known as a probabilistic classifier, is based on Bayes Theorem with an independence assumption among predictors. Bayes theorem in simple terms involves making an initial assumption of probability (prior probability), and then conditionally updating this probability based on new data or evidence. This prior probability is sequentially updated when new evidence emerges.  

Naive Bayes classifier makes two key assumptions: all predictors are conditionally independent or unrelated, and all predictors contribute equally to the outcome or response. Although these assumptions do not hold well in real-world, they are computationally fast and have shown to have good accuracy in predictions. We wanted to see how naive bayes model works on our data.  

Here the naive bayes method was used to fit the data with the following parameters:  
+ fL values 0 and 0.5  
+ useKernel = TRUE  
+ adjust parameter set to vector (1.0, 2.0, 4.0)
+ cross-validation with 5 folds  
+ log-loss as metric to evaluate model

```{r naive_bayes}
naive_bayes_model <- train( Diabetes_binary ~ .,
                    data = diabetes_train,
                    method = "nb",
                    metric = "logLoss",
                    trControl = trainControl(method = "cv", number = 5,
                                             summaryFunction = mnLogLoss,
                                             classProbs = TRUE),
                    tuneGrid = expand.grid(fL = c(0,0.5), usekernel = TRUE, adjust = c(1.0, 2.0, 4.0) ))
  
naive_bayes_model
```

The plot below shows the model performance.
```{r}
plot(naive_bayes_model)

```

The best naive bayes model has parameters: fl- `{r} naive_bayes_model$bestTune$fL`, and adjust- `{r} naive_bayes_model$bestTune$adjust` with a logLoss of `{r} min(naive_bayes_model$results$logLoss))`.


## Final Model Selection 

For this section we have compiled the model output results of the 6 best models so far. We then applied these model fits on test data set to see their performance.  

### Output from Model Comparison
Model output results are (not applied to test data set yet):

```{r}
# Best model from the first three models (forward, backward, and glm)
model_name <- names(result1)[apply(result1, MARGIN = 1, FUN = which.min)]
value <- min(result1)

result2 <- data.frame(model_name = value, 
            LASSO = min(lasso_fit$results$logLoss),
            Classification_Tree = min( classification_tree$results$logLoss),
            Random_Forest = min( rf_model$results$logLoss),
            Partial_Least_Squares = min( pls_model$results$logLoss),
            Naive_Bayes = min(naive_bayes_model$results$logLoss))

colnames(result2)[1] <- model_name
result2[1,]

```

Based on the table above `{r} names(result2)[apply(result2, MARGIN = 1, FUN = which.min)]` is the best model with a logloss value of `{r} min(result2)`

### Application of Model Fits on Test Data
Here the 6 best model fits used on test data set and results compared.

```{r}
model_pred <- predict(fit12, newdata = bike_test)
lasso_pred <- predict(lasso_fit, newdata = diabetes_test)
classification_tree_pred <- predict(classification_tree, newdata = diabetes_test)
rf_pred <- predict(rf_model, newdata = diabetes_test)
pls_pred <- predict(rf_model, newdata = diabetes_test)


result3 <- data.frame(model_name = 1,
                      LASSO = LogLoss)

```

